{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34852ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import random\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torcheval.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd7af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random_seed(value=0):\n",
    "    random.seed(value)\n",
    "    np.random.seed(value)\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed(value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def copy_data_to_device(data, device):\n",
    "    if torch.is_tensor(data):\n",
    "        return data.to(device)\n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        return [copy_data_to_device(elem, device) for elem in data]\n",
    "    raise ValueError('Недопустимый тип данных {}'.format(type(data)))\n",
    "\n",
    "\n",
    "def print_grad_stats(model):\n",
    "    mean = 0\n",
    "    std = 0\n",
    "    norm = 1e-5\n",
    "    for param in model.parameters():\n",
    "        grad = getattr(param, 'grad', None)\n",
    "        if grad is not None:\n",
    "            mean += grad.data.abs().mean()\n",
    "            std += grad.data.std()\n",
    "            norm += 1\n",
    "    mean /= norm\n",
    "    std /= norm\n",
    "    print(f'Mean grad {mean}, std {std}, n {norm}')\n",
    "\n",
    "def min_max_mse_loss(input, target, false_target):\n",
    "    return F.mse_loss(input, target) + 1/F.mse_loss(input, false_target)\n",
    "\n",
    "def train_eval_loop(model, train_dataset, val_dataset, criterion,\n",
    "                    lr=1e-4, epoch_n=10, batch_size=32,\n",
    "                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n",
    "                    max_batches_per_epoch_train=10000,\n",
    "                    max_batches_per_epoch_val=1000,\n",
    "                    data_loader_ctor=DataLoader,\n",
    "                    optimizer_ctor=None,\n",
    "                    lr_scheduler_ctor=None,\n",
    "                    shuffle_train=True,\n",
    "                    plot=False,\n",
    "                    dataloader_workers_n=0):\n",
    "    \"\"\"\n",
    "    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n",
    "    :param model: torch.nn.Module - обучаемая модель\n",
    "    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n",
    "    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n",
    "    :param criterion: функция потерь для настройки модели\n",
    "    :param lr: скорость обучения\n",
    "    :param epoch_n: максимальное количество эпох\n",
    "    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n",
    "    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n",
    "    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n",
    "        отсутствие улучшения модели, чтобы обучение продолжалось.\n",
    "    :param l2_reg_alpha: коэффициент L2-регуляризации\n",
    "    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n",
    "    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n",
    "    :param data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n",
    "        (по умолчанию torch.utils.data.DataLoader)\n",
    "    :return: кортеж из двух элементов:\n",
    "        - среднее значение функции потерь на валидации на лучшей эпохе\n",
    "        - лучшая модель\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "\n",
    "    if optimizer_ctor is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n",
    "    else:\n",
    "        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n",
    "\n",
    "    if lr_scheduler_ctor is not None:\n",
    "        lr_scheduler = lr_scheduler_ctor(optimizer)\n",
    "    else:\n",
    "        lr_scheduler = None\n",
    "\n",
    "    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n",
    "                                        num_workers=dataloader_workers_n)\n",
    "    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                                      num_workers=dataloader_workers_n)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch_i = 0\n",
    "    best_model = copy.deepcopy(model)\n",
    "\n",
    "    for epoch_i in range(epoch_n):\n",
    "        try:\n",
    "            epoch_start = datetime.datetime.now()\n",
    "            print('Эпоха {}'.format(epoch_i))\n",
    "\n",
    "            model.train()\n",
    "            mean_train_loss = 0\n",
    "            train_batches_n = 0\n",
    "            for batch_i, (batch_x, batch_y, batch_false) in enumerate(train_dataloader):\n",
    "                if batch_i > max_batches_per_epoch_train:\n",
    "                    break\n",
    "\n",
    "                batch_x = copy_data_to_device(batch_x, device)\n",
    "                batch_y = copy_data_to_device(batch_y, device)\n",
    "                batch_false = copy_data_to_device(batch_false, device)\n",
    "\n",
    "                pred = model(batch_x)\n",
    "                loss = criterion(pred, batch_y, batch_false)\n",
    "\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                mean_train_loss += float(loss)\n",
    "                train_batches_n += 1\n",
    "\n",
    "            mean_train_loss /= train_batches_n\n",
    "            print('Эпоха: {} итераций, {:0.2f} сек'.format(train_batches_n,\n",
    "                                                           (datetime.datetime.now() - epoch_start).total_seconds()))\n",
    "            print('Среднее значение функции потерь на обучении', mean_train_loss)\n",
    "\n",
    "\n",
    "\n",
    "            model.eval()\n",
    "            mean_val_loss = 0\n",
    "            val_batches_n = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_i, (batch_x, batch_y, batch_false) in enumerate(val_dataloader):\n",
    "                    if batch_i > max_batches_per_epoch_val:\n",
    "                        break\n",
    "\n",
    "                    batch_x = copy_data_to_device(batch_x, device)\n",
    "                    batch_y = copy_data_to_device(batch_y, device)\n",
    "                    batch_false = copy_data_to_device(batch_false, device)\n",
    "\n",
    "                    pred = model(batch_x)\n",
    "                    loss = criterion(pred, batch_y, batch_false)\n",
    "\n",
    "                    mean_val_loss += float(loss)\n",
    "                    val_batches_n += 1\n",
    "\n",
    "            mean_val_loss /= val_batches_n\n",
    "            print('Среднее значение функции потерь на валидации', mean_val_loss)\n",
    "\n",
    "            if mean_val_loss < best_val_loss:\n",
    "                best_epoch_i = epoch_i\n",
    "                best_val_loss = mean_val_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "                print('Новая лучшая модель!')\n",
    "            elif epoch_i - best_epoch_i > early_stopping_patience:\n",
    "                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n",
    "                    early_stopping_patience))\n",
    "                break\n",
    "\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step(mean_val_loss)\n",
    "\n",
    "            print()\n",
    "        except KeyboardInterrupt:\n",
    "            print('Досрочно остановлено пользователем')\n",
    "            break\n",
    "        except Exception as ex:\n",
    "            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n",
    "            break\n",
    "\n",
    "    return best_val_loss, best_model\n",
    "\n",
    "\n",
    "def predict_with_model(model, dataset, device=None, batch_size=32, num_workers=0, return_labels=False):\n",
    "    \"\"\"\n",
    "    :param model: torch.nn.Module - обученная модель\n",
    "    :param dataset: torch.utils.data.Dataset - данные для применения модели\n",
    "    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n",
    "    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n",
    "    :return: numpy.array размерности len(dataset) x *\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    results_by_batch = []\n",
    "\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        import tqdm\n",
    "        for batch_x, batch_y, batch_false in tqdm.tqdm(dataloader, total=len(dataset)/batch_size):\n",
    "            batch_x = copy_data_to_device(batch_x, device)\n",
    "\n",
    "            if return_labels:\n",
    "                labels.append(batch_y.numpy())\n",
    "\n",
    "            batch_pred = model(batch_x)\n",
    "            results_by_batch.append(batch_pred.detach().cpu().numpy())\n",
    "\n",
    "    if return_labels:\n",
    "        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n",
    "    else:\n",
    "        return np.concatenate(results_by_batch, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1331da3",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e6bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/tmnist-typeface-mnist/TMNIST_Data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61102cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,2:]\n",
    "y = df['labels']\n",
    "dict_of_labels = dict(zip(df['labels'].unique(), df['names'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daa752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dddb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 2, 5\n",
    "fig, ax = plt.subplots(rows, cols, figsize = (cols  * 3, rows * 3))\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        num_class = j + cols * i\n",
    "        class_series = df.query(\"labels == @num_class\").iloc[0, :]\n",
    "        ax[i, j].imshow(np.array(class_series[2:], dtype=float).reshape(28,28), cmap=\"binary\")\n",
    "        ax[i, j].set_title(f\"{class_series.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf60edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = df.iloc[np.random.choice(len(df), 10, replace=False)]\n",
    "rows, cols = 2, 5\n",
    "fig, ax = plt.subplots(rows, cols, figsize = (cols  * 3, rows * 3))\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        num_row = j + cols * i\n",
    "        class_series = subdf.iloc[num_row, :]\n",
    "        ax[i, j].imshow(np.array(class_series.iloc[2:], dtype=float).reshape(28,28), cmap=\"binary\")\n",
    "        ax[i, j].set_title(f\"{class_series.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e56b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, _, _ =  train_test_split(df, df['labels'], test_size=0.4, stratify=df['labels'])\n",
    "df_test, df_val, _, _ = train_test_split(df_test, df_test['labels'], test_size=0.5, stratify=df_test['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7273b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TMNIST_dataset(Dataset):\n",
    "    def __init__(self, pd_df):\n",
    "        self.dataset = pd_df.reset_index(drop=True)\n",
    "        self.labels = pd_df[[\"names\", \"labels\"]]\n",
    "        self.dataset = self.dataset.drop([\"names\", \"labels\"], axis=1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        randint = np.random.randint(0, len(self.dataset), 1)[0]\n",
    "        img_vector = torch.tensor(self.dataset.iloc[index].to_numpy(), dtype=torch.float32)\n",
    "        false_vector = torch.tensor(self.dataset.iloc[randint].to_numpy(), dtype=torch.float32)\n",
    "        return [img_vector, img_vector, false_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75feb385",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TMNIST_dataset(df_train)\n",
    "validation_dataset = TMNIST_dataset(df_val)\n",
    "test_dataset = TMNIST_dataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3747bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TMNIST_model(nn.Module):\n",
    "    def __init__(self, in_shape = 784, n_components = 2):\n",
    "        super(TMNIST_model, self).__init__()\n",
    "        \n",
    "        self.activation = nn.SELU()\n",
    "        self.in_batchnorm = nn.BatchNorm1d(in_shape)\n",
    "        self.linear_11 = nn.Linear(in_shape, 400)\n",
    "        self.linear_12 = nn.Linear(400, 200)\n",
    "        self.linear_13 = nn.Linear(200, 50)\n",
    "        self.linear_14 = nn.Linear(50, n_components)\n",
    "        self.hidden_batchnorm = nn.BatchNorm1d(n_components)\n",
    "        \n",
    "        self.linear_21 = nn.Linear(n_components, 50)\n",
    "        self.linear_22 = nn.Linear(50, 200)\n",
    "        self.linear_23 = nn.Linear(200, 400)\n",
    "        self.batchnorm_23 = nn.BatchNorm1d(400)\n",
    "        self.linear_24 = nn.Linear(400, in_shape)\n",
    "        \n",
    "        self.fc_encoder = nn.Sequential(\n",
    "            self.in_batchnorm,\n",
    "            self.linear_11,\n",
    "            self.activation,\n",
    "            self.linear_12,\n",
    "            self.activation,\n",
    "            self.linear_13,\n",
    "            self.activation,\n",
    "            self.linear_14,\n",
    "            self.hidden_batchnorm,\n",
    "        )\n",
    "        \n",
    "        self.fc_decoder = nn.Sequential(\n",
    "            self.linear_21,\n",
    "            self.activation,\n",
    "            self.linear_22,\n",
    "            self.activation,\n",
    "            self.linear_23,\n",
    "            self.batchnorm_23,\n",
    "            self.activation,\n",
    "            self.linear_24\n",
    "        )\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x = self.fc_encoder(data)\n",
    "        x = self.fc_decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e57237",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = TMNIST_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8f56a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss, best_model = train_eval_loop(model=model_1, \n",
    "                train_dataset=train_dataset, \n",
    "                val_dataset=validation_dataset, \n",
    "                criterion=min_max_mse_loss,\n",
    "                lr=1e-2, \n",
    "                epoch_n=200, \n",
    "                batch_size=len(train_dataset)//50,\n",
    "                device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\", \n",
    "                early_stopping_patience=20, \n",
    "                l2_reg_alpha=0.15,\n",
    "                max_batches_per_epoch_train=10000,\n",
    "                max_batches_per_epoch_val=1000,\n",
    "                data_loader_ctor=DataLoader,\n",
    "                optimizer_ctor=torch.optim.Adam,\n",
    "                lr_scheduler_ctor=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                shuffle_train=True,\n",
    "                dataloader_workers_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict_with_model(best_model.fc_encoder, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb0493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f74ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(result[:, 0], result[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56962549",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.labels[\"names\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f641c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Границы отображения\n",
    "x_lim = (-6, 6)\n",
    "y_lim = (-6, 6)\n",
    "grid_step = 0.5\n",
    "grid_x = np.arange(x_lim[0], x_lim[1] + grid_step, grid_step).round(1)\n",
    "grid_y = np.arange(y_lim[0], y_lim[1] + grid_step, grid_step).round(1)\n",
    "\n",
    "\n",
    "# Отображение\n",
    "plt.figure(figsize=(9,8), dpi=100)\n",
    "plt.title(\"Дексрипторы записей со скрытого слоя\", fontsize=16)\n",
    "sns.scatterplot(x=result[:, 0], y=result[:, 1], hue=test_dataset.labels[\"labels\"])\n",
    "plt.xlabel(\"Первый выход промежуточного слоя\", fontsize=16)\n",
    "plt.ylabel(\"Второй выход промежуточного слоя\", fontsize=16)\n",
    "plt.xlim(x_lim)\n",
    "plt.ylim(y_lim)\n",
    "plt.xticks(grid_x, grid_x, fontsize=16, rotation=40)\n",
    "plt.yticks(grid_y, grid_y, fontsize=16)\n",
    "plt.axvline(0, y_lim[0], y_lim[1], linewidth=1, color=\"black\")\n",
    "plt.grid(linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96073463",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_out = predict_with_model(best_model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_num = 10\n",
    "random_num = np.random.choice(np.arange(0, len(test_dataset)), images_num)\n",
    "\n",
    "fig, ax = plt.subplots(images_num, 2, figsize=(12, 6 * images_num))\n",
    "for image in range(images_num):\n",
    "    random_image = random_num[image]\n",
    "    ax[image, 0].imshow(test_dataset[random_image][0].reshape(28, 28))\n",
    "    ax[image, 1].imshow(images_out[random_image].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a4cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (logo_venv)",
   "language": "python",
   "name": "logo-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
